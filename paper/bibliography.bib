% FPC-AE1r Bibliography
% Author: Aleksei Novgorodtsev (AIDoctrine)

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and others},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023}
}

@inproceedings{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and others},
  booktitle={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  journal={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017}
}

@article{openai2023gpt4,
  title={GPT-4 technical report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anthropic2024claude,
  title={The Claude 3 model family: Opus, Sonnet, Haiku},
  author={{Anthropic}},
  journal={Technical Report},
  year={2024},
  url={https://www.anthropic.com/claude}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and others},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{olah2020zoom,
  title={Zoom in: An introduction to circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and others},
  journal={Distill},
  volume={5},
  number={3},
  pages={e00024--001},
  year={2020}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{templeton2024scaling,
  title={Scaling monosemanticity: Extracting interpretable features from Claude 3 Sonnet},
  author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and others},
  journal={Anthropic Research},
  year={2024}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{azaria2023internal,
  title={The internal state of an LLM knows when it's lying},
  author={Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2304.13734},
  year={2023}
}

@article{manheim2019categorizing,
  title={Categorizing variants of Goodhart's law},
  author={Manheim, David and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1803.04585},
  year={2019}
}

@article{sharma2023towards,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}

@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  edition={3rd},
  year={2016},
  publisher={Pearson}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and others},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

% SimpleQA dataset reference
@misc{simpleqa2024,
  title={SimpleQA: A benchmark for evaluating factuality in question answering},
  author={{OpenAI}},
  year={2024},
  howpublished={\url{https://openai.com/index/introducing-simpleqa/}},
  note={Accessed: 2025-10}
}
